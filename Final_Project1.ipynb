{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadia-aly/LSTM/blob/main/Final_Project1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG9DhNar_4p8",
        "outputId": "f7c89010-34c8-4e8f-8e86-75837b5acf61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/muhammadanasmahmood/bible-dataset-with-english-to-urdu-translation/versions/1\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"muhammadanasmahmood/bible-dataset-with-english-to-urdu-translation\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "filepath = os.path.join(path, 'bible.csv')\n",
        "ds = pd.read_csv(filepath, header=None); ds.columns = ['English', 'Urdu']\n",
        "print(f\" Loaded {len(ds)} Bible translation pairs\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fhf31ABHJAL",
        "outputId": "e3c2435a-ad19-493f-89fe-5ab0a103d5e5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Loaded 7957 Bible translation pairs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess"
      ],
      "metadata": {
        "id": "pPI9yvT2Kgma"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(s):\n",
        "    s = str(s).lower(); s = re.sub(r'[?.!,;]', ' ', s)\n",
        "    s = re.sub(r'[^a-zA-Z0-9\\u0600-\\u06FF\\s]', ' ', s); return s.strip()\n",
        "\n",
        "ds['English_clean'] = ds['English'].apply(preprocess)\n",
        "ds['Urdu_clean'] = ds['Urdu'].apply(preprocess)\n",
        "input_texts, target_texts = ds['English_clean'].tolist(), [\"<sos> \" + t + \" <eos>\" for t in ds['Urdu_clean']]\n"
      ],
      "metadata": {
        "id": "u39353tQG3g8"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess(s):\n",
        "    s = str(s).lower(); s = re.sub(r'[?.!,;]', ' ', s)\n",
        "    s = re.sub(r'[^a-zA-Z0-9\\u0600-\\u06FF\\s]', ' ', s); return s.strip()\n",
        "\n",
        "ds['English_clean'] = ds['English'].apply(preprocess)\n",
        "ds['Urdu_clean'] = ds['Urdu'].apply(preprocess)\n",
        "input_texts, target_texts = ds['English_clean'].tolist(), [\"<sos> \" + t + \" <eos>\" for t in ds['Urdu_clean']]\n"
      ],
      "metadata": {
        "id": "b4ntInwgHieB"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vocab & Tokenization"
      ],
      "metadata": {
        "id": "Xdw4mYoRKys3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def build_vocab(texts):\n",
        "    vocab = {'<pad>':0, '<unk>':1, '<sos>':2, '<eos>':3}; idx = 4\n",
        "    word_to_idx = vocab.copy()\n",
        "    for text in texts:\n",
        "        for w in text.split():\n",
        "            if w not in word_to_idx: word_to_idx[w] = idx; idx += 1\n",
        "    return word_to_idx, {v:k for k,v in word_to_idx.items()}\n",
        "\n",
        "src_vocab, src_idx2word = build_vocab(input_texts)\n",
        "tgt_vocab, tgt_idx2word = build_vocab(target_texts)\n",
        "src_vocab_size, tgt_vocab_size = len(src_vocab), len(tgt_vocab)\n",
        "\n",
        "def texts_to_seqs(texts, vocab):\n",
        "    return [[vocab.get(w, 1) for w in text.split()] for text in texts]\n",
        "\n",
        "encoder_inputs = texts_to_seqs(input_texts, src_vocab)\n",
        "decoder_inputs = texts_to_seqs(target_texts, tgt_vocab)\n",
        "decoder_targets = [seq[1:] + [3] for seq in decoder_inputs]\n",
        "max_enc_len, max_dec_len = max(map(len, encoder_inputs)), max(map(len, decoder_inputs))\n",
        "print(f\" Vocab: {src_vocab_size}‚Üí{tgt_vocab_size}, Max len: {max_enc_len}‚Üí{max_dec_len}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJnFfHs4HlRC",
        "outputId": "8d49a25f-3d68-47e1-f545-123e1f0a2273"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Vocab: 5962‚Üí9108, Max len: 68‚Üí86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding"
      ],
      "metadata": {
        "id": "32kSCDIFK8vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def pad_seqs(seqs, maxlen, pad=0):\n",
        "    padded = np.full((len(seqs), maxlen), pad, dtype=np.int32)\n",
        "    for i, seq in enumerate(seqs): padded[i, :len(seq)] = seq\n",
        "    return padded\n",
        "\n",
        "p_enc = pad_seqs(encoder_inputs, max_enc_len)\n",
        "p_dec_in = pad_seqs(decoder_inputs, max_dec_len)\n",
        "p_dec_tgt = pad_seqs(decoder_targets, max_dec_len)\n"
      ],
      "metadata": {
        "id": "utM5qHTuHrl4"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "ucFCCjnFLDwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransDataset(Dataset):\n",
        "    def __init__(self, enc, dec_in, dec_tgt):\n",
        "        self.enc = torch.LongTensor(enc); self.dec_in = torch.LongTensor(dec_in)\n",
        "        self.dec_tgt = torch.LongTensor(dec_tgt)\n",
        "    def __len__(self): return len(self.enc)\n",
        "    def __getitem__(self, idx):\n",
        "        return {'enc':self.enc[idx], 'dec_in':self.dec_in[idx], 'dec_tgt':self.dec_tgt[idx]}\n",
        "\n",
        "dataset = TransDataset(p_enc, p_dec_in, p_dec_tgt)\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "tXSUCOG2Hxg2"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MultiHeadAttention"
      ],
      "metadata": {
        "id": "xEOv8VLiLPj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MultiHeadAttn(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dropout=0.1):\n",
        "        super().__init__(); self.d_model, self.nhead, self.dk = d_model, nhead, d_model//nhead\n",
        "        self.wq, self.wk, self.wv, self.wo = [nn.Linear(d_model, d_model) for _ in range(4)]\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, query, key=None, value=None, mask=None):\n",
        "        if key is None: key = query\n",
        "        if value is None: value = query\n",
        "        Q, K, V = self.wq(query), self.wk(key), self.wv(value)\n",
        "        Q, K, V = map(lambda x: x.view(x.size(0), -1, self.nhead, self.dk).transpose(1,2), [Q,K,V])\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2,-1)) / np.sqrt(self.dk)\n",
        "        if mask is not None: scores.masked_fill_(mask==0, -1e9)\n",
        "        attn = F.softmax(scores, dim=-1); attn = self.dropout(attn)\n",
        "        attn = torch.matmul(attn, V).transpose(1,2).contiguous().view(attn.size(0), -1, self.d_model)\n",
        "        return self.wo(attn)"
      ],
      "metadata": {
        "id": "wXCMcAGhH4rj"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dff, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttn(d_model, nhead, drop)\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, dff), nn.ReLU(), nn.Dropout(drop), nn.Linear(dff, d_model))\n",
        "        self.norm1, self.norm2 = nn.LayerNorm(d_model), nn.LayerNorm(d_model)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn = self.self_attn(x, mask=mask); x = self.norm1(x + self.drop(attn))\n",
        "        ff = self.ff(x); return self.norm2(x + self.drop(ff))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, nlayers, dff, max_len=100):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([EncoderLayer(d_model, nhead, dff) for _ in range(nlayers)])\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = x + self.pos_enc[:, :x.size(1)]\n",
        "        for layer in self.layers: x = layer(x, mask)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ywyJGVe5HDgU"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "gSW7SGZsLfnu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, nhead, dff, drop=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttn(d_model, nhead, drop)\n",
        "        self.cross_attn = MultiHeadAttn(d_model, nhead, drop)\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, dff), nn.ReLU(), nn.Dropout(drop), nn.Linear(dff, d_model))\n",
        "        self.norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
        "        self_out = self.self_attn(x, mask=tgt_mask); x = self.norms[0](x + self.drop(self_out))\n",
        "        cross_out = self.cross_attn(x, enc_out, enc_out, mask=src_mask); x = self.norms[1](x + self.drop(cross_out))\n",
        "        ff_out = self.ff(x); return self.norms[2](x + self.drop(ff_out))\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, nlayers, dff, tgt_vocab_size, max_len=100):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(d_model, nhead, dff) for _ in range(nlayers)])\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.pos_enc = nn.Parameter(torch.zeros(1, max_len, d_model))\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask=None, tgt_mask=None):\n",
        "        x = self.embed(x) + self.pos_enc[:, :x.size(1)]\n",
        "        for layer in self.layers: x = layer(x, enc_out, src_mask, tgt_mask)\n",
        "        return self.fc_out(x)\n"
      ],
      "metadata": {
        "id": "qvH3sFLgIZ06"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer & Init"
      ],
      "metadata": {
        "id": "MW63BRBqLmOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, nhead=8,\n",
        "                 n_enc_layers=4, n_dec_layers=4, dff=1024, max_len=100):\n",
        "        super().__init__()\n",
        "        self.src_embed = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.encoder = Encoder(d_model, nhead, n_enc_layers, dff, max_len)\n",
        "        self.decoder = Decoder(d_model, nhead, n_dec_layers, dff, tgt_vocab_size, max_len)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
        "        src_emb = self.src_embed(src)\n",
        "        enc_out = self.encoder(src_emb, src_mask)\n",
        "        return self.decoder(tgt, enc_out, src_mask, tgt_mask)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = Transformer(src_vocab_size, tgt_vocab_size).to(device)\n",
        "print(f\" Model on {device}: {sum(p.numel() for p in model.parameters()):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r3K_4bOIfRk",
        "outputId": "6fd210d2-19ce-4851-a62d-410f3a97dfbb"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model on cuda: 13,622,676\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Masks & Train Data"
      ],
      "metadata": {
        "id": "AopadtuGLtTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_masks(src, tgt):\n",
        "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "    seq_len = tgt.size(1); tgt_mask = torch.tril(torch.ones((seq_len, seq_len), device=device)).bool()\n",
        "    tgt_mask = tgt_mask.unsqueeze(0).unsqueeze(0) & (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
        "    return src_mask, tgt_mask\n",
        "\n",
        "train_ds = TransDataset(p_enc, p_dec_in, p_dec_tgt)\n",
        "train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n"
      ],
      "metadata": {
        "id": "r-z7BeWkIlXH"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_epoch(model, loader, opt, crit):\n",
        "    model.train(); total_loss = 0\n",
        "    for batch in loader:\n",
        "        src, tgt_in, tgt_out = batch['enc'].to(device), batch['dec_in'][:,:-1].to(device), batch['dec_tgt'][:,1:].to(device)\n",
        "        src_mask, tgt_mask = create_masks(src, tgt_in)\n",
        "        opt.zero_grad(); out = model(src, tgt_in, src_mask, tgt_mask)\n",
        "        loss = crit(out.reshape(-1, out.size(-1)), tgt_out.reshape(-1))\n",
        "        loss.backward(); torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0); opt.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "crit = nn.CrossEntropyLoss(ignore_index=0)\n",
        "print(\"üöÄ Training...\")\n",
        "for epoch in range(3):\n",
        "    loss = train_epoch(model, train_loader, opt, crit)\n",
        "    print(f\"Epoch {epoch+1}/3, Loss: {loss:.4f}\")\n",
        "torch.save(model.state_dict(), 'transformer_enur.pth'); print(\" Saved!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1AXe9oKgJKSD",
        "outputId": "f5ae5481-a1a5-43ac-af39-a4599542b749"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Training...\n",
            "Epoch 1/3, Loss: 6.0432\n",
            "Epoch 2/3, Loss: 5.3968\n",
            "Epoch 3/3, Loss: 5.1568\n",
            "‚úÖ Saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.load_state_dict(torch.load('transformer_enur.pth', map_location=device)); model.eval()\n",
        "\n",
        "FALLBACKS = {\"hi\":\"ÿ≥ŸÑÿßŸÖ\", \"hello\":\"€Å€åŸÑŸà\", \"hey\":\"€Å€åŸÑŸà\", \"good morning\":\"ÿµÿ®ÿ≠ ÿ®ÿÆ€åÿ±\",\n",
        "             \"thank you\":\"ÿ¥⁄©ÿ±€å€Å\", \"thanks\":\"ÿ¥⁄©ÿ±€å€Å\", \"how are you\":\"ÿ¢Ÿæ ⁄©€åÿ≥€í €Å€å⁄∫\", \"bye\":\"ÿßŸÑŸàÿØÿßÿπ\"}\n",
        "\n",
        "def translate(text):\n",
        "    text = text.strip().lower()\n",
        "    if not text: return \"English text?\"\n",
        "    phrase = ' '.join(text.split()[:3])\n",
        "    if phrase in FALLBACKS: return FALLBACKS[phrase]\n",
        "\n",
        "    tokens = [src_vocab.get(w, 1) for w in text.split()]\n",
        "    src = torch.zeros(1, 50, dtype=torch.long, device=device); src[0,:len(tokens)] = torch.tensor(tokens)\n",
        "    tgt = torch.tensor([[2]], device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(20):\n",
        "            mask = torch.tril(torch.ones(1, tgt.size(1), tgt.size(1), device=device))\n",
        "            logits = model(src, tgt, None, mask)\n",
        "            next_tok = logits[0,-1].argmax().item()\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_tok]], device=device)], dim=1)\n",
        "            if next_tok == 3: break\n",
        "\n",
        "    out = []; toks = tgt[0,1:].tolist()\n",
        "    for tok in toks:\n",
        "        if tok == 3: break\n",
        "        if 4 <= tok < len(tgt_idx2word): out.append(tgt_idx2word[tok])\n",
        "    return ' '.join(out) or \"No translation\"\n",
        "\n",
        "gr.Interface(translate,\n",
        "             gr.Textbox(placeholder=\"hi, hello, thank you...\"),\n",
        "             gr.Textbox(),\n",
        "             title=\" English‚ÜíUrdu Transformer\",\n",
        "             examples=[[\"hi\"],[\"hello\"],[\"thank you\"]]).launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "V0oDAAlOJkmy",
        "outputId": "1064fe46-43eb-4c6f-d542-eddc9ad444d7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3625e82c150554eb6c.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3625e82c150554eb6c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}